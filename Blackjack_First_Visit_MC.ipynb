{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Blackjack_First_Visit_MC",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IZ2igsBMwo4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg2QJXw8OSgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import enum\n",
        "\n",
        "ranks = {\n",
        "    \"two\" : 2,\n",
        "    \"three\" : 3,\n",
        "    \"four\" : 4,\n",
        "    \"five\" : 5,\n",
        "    \"six\" : 6,\n",
        "    \"seven\" : 7,\n",
        "    \"eight\" : 8,\n",
        "    \"nine\" : 9,\n",
        "    \"ten\" : 10,\n",
        "    \"jack\" : 10,\n",
        "    \"queen\" : 10,\n",
        "    \"king\" : 10,\n",
        "    \"ace\" : (1, 11)\n",
        "}\n",
        "    \n",
        "class Suit(enum.Enum):\n",
        "    spades = \"spades\"\n",
        "    clubs = \"clubs\"\n",
        "    diamonds = \"diamonds\"\n",
        "    hearts = \"hearts\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0r6gqp9OVwu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Card:\n",
        "    def __init__(self, suit, rank, value):\n",
        "        self.suit = suit\n",
        "        self.rank = rank\n",
        "        self.value = value\n",
        "        \n",
        "    def __str__(self):\n",
        "        return self.rank + \" of \" + self.suit.value\n",
        "\n",
        "class Deck:\n",
        "    def __init__(self, num=1):\n",
        "        self.cards = []\n",
        "        for i in range(num):\n",
        "            for suit in Suit:\n",
        "                for rank, value in ranks.items():\n",
        "                    self.cards.append(Card(suit, rank, value))\n",
        "                \n",
        "    def shuffle(self):\n",
        "        random.shuffle(self.cards)\n",
        "        \n",
        "    def deal(self):\n",
        "        return self.cards.pop(0)\n",
        "    \n",
        "    def peek(self):\n",
        "        if len(self.cards) > 0:\n",
        "            return self.cards[0]\n",
        "        \n",
        "    def add_to_bottom(self, card):\n",
        "        self.cards.append(card)\n",
        "        \n",
        "    def __str__(self):\n",
        "        result = \"\"\n",
        "        for card in self.cards:\n",
        "            result += str(card) + \"\\n\"\n",
        "        return result\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.cards)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6geWMqLAObzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This follows the same, official rules every time.\n",
        "# Still need to figure out what happens if there are multiple Aces.\n",
        "def dealer_eval(player_hand):\n",
        "    num_ace = 0\n",
        "    use_one = 0\n",
        "    for card in player_hand:\n",
        "        if card.rank == \"ace\":\n",
        "            num_ace += 1\n",
        "            use_one += card.value[0] # use 1 for Ace\n",
        "        else:\n",
        "            use_one += card.value\n",
        "    \n",
        "    if num_ace > 0:\n",
        "        # See if using 11 instead of 1 for the Aces gets the \n",
        "        # dealer's hand value closer to the [17, 21] range\n",
        "        \n",
        "        # The dealer will follow Hard 17 rules.\n",
        "        # This means the dealer will not hit again if\n",
        "        # the Ace yields a 17. \n",
        "        \n",
        "        # This also means that Aces initially declared as 11's can\n",
        "        # be changed to 1's as new cards come.\n",
        "        \n",
        "        ace_counter = 0\n",
        "        while ace_counter < num_ace:\n",
        "            # Only add by 10 b/c 1 is already added before\n",
        "            use_eleven = use_one + 10 \n",
        "            \n",
        "            if use_eleven > 21:\n",
        "                return use_one\n",
        "            elif use_eleven >= 17 and use_eleven <= 21:\n",
        "                return use_eleven\n",
        "            else:\n",
        "                # The case where even using Ace as eleven is less than 17.\n",
        "                use_one = use_eleven\n",
        "            \n",
        "            ace_counter += 1\n",
        "        \n",
        "        return use_one\n",
        "    else:\n",
        "        return use_one"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ofrXW48OgrP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def player_eval(player_hand):\n",
        "    num_ace = 0\n",
        "    # use_one means that every ace that in the hand is counted as one.\n",
        "    use_one = 0\n",
        "    for card in player_hand:\n",
        "        if card.rank == \"ace\":\n",
        "            num_ace += 1\n",
        "            use_one += card.value[0] # use 1 for Ace\n",
        "        else:\n",
        "            use_one += card.value\n",
        "    \n",
        "    if num_ace > 0:\n",
        "        # Define player policy for Aces:\n",
        "        # Make Aces 11 if they get you to the range [18,21]\n",
        "        # Otherwise, use one.\n",
        "        \n",
        "        ace_counter = 0\n",
        "        while ace_counter < num_ace:\n",
        "            # Only add by 10 b/c 1 is already added before\n",
        "            use_eleven = use_one + 10 \n",
        "            \n",
        "            if use_eleven > 21:\n",
        "                return use_one\n",
        "            elif use_eleven >= 18 and use_eleven <= 21:\n",
        "                return use_eleven\n",
        "            else:\n",
        "                # This allows for some Aces to be 11s, and others to be 1.\n",
        "                use_one = use_eleven\n",
        "            \n",
        "            ace_counter += 1\n",
        "        \n",
        "        return use_one\n",
        "    else:\n",
        "        return use_one"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHHJGyzCOlZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dealer_turn(dealer_hand, deck):\n",
        "    # Calculate dealer hand's value.\n",
        "    dealer_value = dealer_eval(dealer_hand)\n",
        "\n",
        "    # Define dealer policy (is fixed to official rules)\n",
        "\n",
        "    # The dealer keeps hitting until their total is 17 or more\n",
        "    while dealer_value < 17:\n",
        "        # hit\n",
        "        dealer_hand.append(deck.deal())\n",
        "        dealer_value = dealer_eval(dealer_hand)\n",
        "\n",
        "    return dealer_value, dealer_hand, deck"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EXfv6AFOsFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym import error, spaces, utils\n",
        "from gym.utils import seeding\n",
        "\n",
        "INITIAL_BALANCE = 1000\n",
        "NUM_DECKS = 6\n",
        "\n",
        "class BlackjackEnv(gym.Env):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(BlackjackEnv, self).__init__()\n",
        "        \n",
        "        # Initialize the blackjack deck.\n",
        "        self.bj_deck = Deck(NUM_DECKS)\n",
        "        \n",
        "        self.player_hand = []\n",
        "        self.dealer_hand = []\n",
        "        \n",
        "        self.reward_options = {\"lose\":-100, \"tie\":0, \"win\":100}\n",
        "        \n",
        "        # hit = 0, stand = 1\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "        \n",
        "        '''\n",
        "        First element of tuple is the range of possible hand values for the player. (3 through 20)\n",
        "        This is the possible range of values that the player will actually have to make a decision for.\n",
        "        Any player hand value 21 or above already has automatic valuations, and needs no input from an\n",
        "        AI Agent. \n",
        "        '''\n",
        "        \n",
        "        # Second element of the tuple is the range of possible values for the dealer's upcard. (2 through 11)\n",
        "        self.observation_space = spaces.Tuple((spaces.Discrete(18), spaces.Discrete(10)))\n",
        "        \n",
        "        self.done = False\n",
        "        \n",
        "    def _take_action(self, action):\n",
        "        if action == 0: # hit\n",
        "            self.player_hand.append(self.bj_deck.deal())\n",
        "            \n",
        "        # re-calculate the value of the player's hand after any changes to the hand.\n",
        "        self.player_value = player_eval(self.player_hand)\n",
        "    \n",
        "    def step(self, action):\n",
        "        self._take_action(action)\n",
        "        \n",
        "        # End the episode/game is the player stands or has a hand value >= 21.\n",
        "        self.done = action == 1 or self.player_value >= 21\n",
        "        \n",
        "        # rewards are 0 when the player hits and is still below 21, and they\n",
        "        # keep playing.\n",
        "        rewards = 0\n",
        "        \n",
        "        if self.done:\n",
        "            # CALCULATE REWARDS\n",
        "            if self.player_value > 21: # above 21, player loses automatically.\n",
        "                rewards = self.reward_options[\"lose\"]\n",
        "            elif self.player_value == 21: # blackjack! Player wins automatically.\n",
        "                rewards = self.reward_options[\"win\"]\n",
        "            else:\n",
        "                ## Begin dealer turn phase.\n",
        "\n",
        "                dealer_value, self.dealer_hand, self.bj_deck = dealer_turn(self.dealer_hand, self.bj_deck)\n",
        "\n",
        "                ## End of dealer turn phase\n",
        "\n",
        "                #------------------------------------------------------------#\n",
        "\n",
        "                ## Final Compare\n",
        "\n",
        "                if dealer_value > 21: # dealer above 21, player wins automatically\n",
        "                    rewards = self.reward_options[\"win\"]\n",
        "                elif dealer_value == 21: # dealer has blackjack, player loses automatically\n",
        "                    rewards = self.reward_options[\"lose\"]\n",
        "                else: # dealer and player have values less than 21.\n",
        "                    if self.player_value > dealer_value: # player closer to 21, player wins.\n",
        "                        rewards = self.reward_options[\"win\"]\n",
        "                    elif self.player_value < dealer_value: # dealer closer to 21, dealer wins.\n",
        "                        rewards = self.reward_options[\"lose\"]\n",
        "                    else:\n",
        "                        rewards = self.reward_options[\"tie\"]\n",
        "        \n",
        "        self.balance += rewards\n",
        "        \n",
        "        \n",
        "        # Subtract by 1 to fit into the possible observation range.\n",
        "        # This makes the possible range of 3 through 20 into 1 through 18\n",
        "        player_value_obs = self.player_value - 2\n",
        "        \n",
        "        # get the value of the dealer's upcard, this value is what the agent sees.\n",
        "        # Subtract by 1 to fit the possible observation range of 1 to 10.\n",
        "        upcard_value_obs = dealer_eval([self.dealer_upcard]) - 1\n",
        "        \n",
        "        # the state is represented as a player hand-value + dealer upcard pair.\n",
        "        obs = np.array([player_value_obs, upcard_value_obs])\n",
        "        \n",
        "        return obs, rewards, self.done, {}\n",
        "    \n",
        "    def reset(self): # resets game to an initial state\n",
        "        # Add the player and dealer cards back into the deck.\n",
        "        self.bj_deck.cards += self.player_hand + self.dealer_hand\n",
        "\n",
        "        # Shuffle before beginning. Only shuffle once before the start of each game.\n",
        "        self.bj_deck.shuffle()\n",
        "         \n",
        "        self.balance = INITIAL_BALANCE\n",
        "        \n",
        "        self.done = False\n",
        "        \n",
        "        # returns the start state for the agent\n",
        "        # deal 2 cards to the agent and the dealer\n",
        "        self.player_hand = [self.bj_deck.deal(), self.bj_deck.deal()]\n",
        "        self.dealer_hand = [self.bj_deck.deal(), self.bj_deck.deal()]\n",
        "        self.dealer_upcard = self.dealer_hand[0]\n",
        "        \n",
        "        # calculate the value of the agent's hand\n",
        "        self.player_value = player_eval(self.player_hand)\n",
        "        \n",
        "        # Subtract by 1 to fit into the possible observation range.\n",
        "        # This makes the possible range of 2 through 20 into 1 through 18\n",
        "        player_value_obs = self.player_value - 2\n",
        "            \n",
        "        # get the value of the dealer's upcard, this value is what the agent sees.\n",
        "        # Subtract by 1 to fit the possible observation range of 1 to 10.\n",
        "        upcard_value_obs = dealer_eval([self.dealer_upcard]) - 1\n",
        "        \n",
        "        # the state is represented as a player hand-value + dealer upcard pair.\n",
        "        obs = np.array([player_value_obs, upcard_value_obs])\n",
        "        \n",
        "        return obs\n",
        "    \n",
        "    def render(self, mode='human', close=False):\n",
        "        # convert the player hand into a format that is\n",
        "        # easy to read and understand.\n",
        "        hand_list = []\n",
        "        for card in self.player_hand:\n",
        "            hand_list.append(card.rank)\n",
        "            \n",
        "        # re-calculate the value of the dealer upcard.\n",
        "        upcard_value = dealer_eval([self.dealer_upcard])\n",
        "        \n",
        "        print(f'Balance: {self.balance}')\n",
        "        print(f'Player Hand: {hand_list}')\n",
        "        print(f'Player Value: {self.player_value}')\n",
        "        print(f'Dealer Upcard: {upcard_value}')\n",
        "        print(f'Done: {self.done}')\n",
        "        \n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDSFV32xO3jl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "14b781e5-8cec-4847-b9ab-dce6c476880b"
      },
      "source": [
        "import random\n",
        "env = BlackjackEnv()\n",
        "\n",
        "total_rewards = 0\n",
        "NUM_EPISODES = 1000\n",
        "\n",
        "for _ in range(NUM_EPISODES):\n",
        "    env.reset()\n",
        "\n",
        "    while env.done == False:\n",
        "        action = env.action_space.sample()\n",
        "\n",
        "        new_state, reward, done, desc = env.step(action)\n",
        "        state = new_state\n",
        "        total_rewards += reward\n",
        "        \n",
        "avg_reward = total_rewards / NUM_EPISODES\n",
        "print(avg_reward)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-41.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGqE1TviO7WR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_mc(env, num_episodes):\n",
        "    '''\n",
        "    observation_space[0] is the 18 possible player values. (3 through 20)\n",
        "    observation_space[1] is the 10 possible dealer upcards. (2 through 11)\n",
        "\n",
        "    Combining these together yields all possible states.\n",
        "\n",
        "    Multiplying this with hit/stand yields all possible state/action pairs.\n",
        "\n",
        "    This is the Q map.\n",
        "    '''\n",
        "    Q = np.zeros([env.observation_space[0].n * env.observation_space[1].n, env.action_space.n], dtype=np.float16)\n",
        "\n",
        "\n",
        "    # This map contains the probability distributions for each action (hit or stand) given a state.\n",
        "    # The state (combo of player hand value and dealer upcard value) index in this array yields a 2-element array\n",
        "    # The 0th index of this 2-element array refers to the probability of \"hit\", and the 1st index is the probability of \"stand\"\n",
        "    prob = np.zeros([env.observation_space[0].n * env.observation_space[1].n, env.action_space.n], dtype=np.float16) + 0.5\n",
        "\n",
        "    # The learning rate. Very small to avoid making quick, large changes in our policy.\n",
        "    alpha = 0.001\n",
        "\n",
        "    epsilon = 1\n",
        "    \n",
        "    # The rate by which epsilon will decay over time.\n",
        "    # Since the probability we take the option with the highest Q-value is 1-epsilon + probability,\n",
        "    # this decay will make sure we are the taking the better option more often in the longrun.\n",
        "    # This allows the algorithm to explore in the early stages, and exploit in the later stages.\n",
        "    decay = 0.9999\n",
        "    # The lowest value that epsilon can go to.\n",
        "    # Although the decay seems slow, it actually grows exponentially, and this is magnified when\n",
        "    # running thousands of episodes.\n",
        "    epsilon_min = 0.9\n",
        "\n",
        "    # may have to be tweaked later.\n",
        "    gamma = 0.8\n",
        "    \n",
        "    for _ in range(num_episodes):\n",
        "        episode = play_game(env, Q, prob)\n",
        "        \n",
        "        epsilon = max(epsilon * decay, epsilon_min)\n",
        "        \n",
        "        Q = update_Q(env, episode, Q, alpha, gamma)\n",
        "        prob = update_prob(env, episode, Q, prob, epsilon)\n",
        "        \n",
        "    return Q, prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fet_o4sIr_1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_mc(env, num_episodes):\n",
        "    Q = np.zeros([env.observation_space[0].n * env.observation_space[1].n, env.action_space.n], dtype=np.float16)\n",
        "    prob = np.zeros([env.observation_space[0].n * env.observation_space[1].n, env.action_space.n], dtype=np.float16) + 0.5\n",
        "    alpha = 0.001\n",
        "    epsilon = 1\n",
        "    decay = 0.9999\n",
        "    epsilon_min = 0.9\n",
        "    # may have to be tweaked later.\n",
        "    gamma = 0.8\n",
        "    \n",
        "    for _ in range(num_episodes):\n",
        "        episode = play_game(env, Q, prob)\n",
        "        \n",
        "        epsilon = max(epsilon * decay, epsilon_min)\n",
        "        \n",
        "        Q = update_Q(env, episode, Q, alpha, gamma)\n",
        "        prob = update_prob(env, episode, Q, prob, epsilon)\n",
        "        \n",
        "    return Q, prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BHJv-QHPOnk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_game(env, Q, prob):\n",
        "    # Can contain numerous state->action->reward tuples because a round of \n",
        "    # Blackjack is not always resolved in one turn.\n",
        "    # However, there will be no state that has a player hand value that exceeds 20, since only initial\n",
        "    # states BEFORE actions are made are used when storing state->action->reward tuples.\n",
        "    episode = []\n",
        "    \n",
        "    state = env.reset()\n",
        "    \n",
        "    while env.done == False:\n",
        "        if state[0] == 19: #Player was dealt Blackjack, player_value already subtracted by 2 to get state[0]\n",
        "            # don't do any episode analysis for this episode. This is a useless episode.\n",
        "            next_state, reward, env.done, info = env.step(1) # doesn't matter what action is taken.\n",
        "        else:\n",
        "            # Get the index in Q that corresponds to the current state\n",
        "            Q_state_index = get_Q_state_index(state)\n",
        "            \n",
        "            # Use the index to get the possible actions, and use np.argmax()\n",
        "            # to get the index of the action that has the highest current Q\n",
        "            # value. Index 0 is hit, index 1 is stand.\n",
        "            best_action = np.argmax(Q[Q_state_index])\n",
        "            \n",
        "            # Go to the prob table to retrieve the probability of this action.\n",
        "            # This uses the same Q_state_index used for finding the state index\n",
        "            # of the Q-array.\n",
        "            prob_of_best_action = get_prob_of_best_action(env, state, Q, prob)\n",
        "\n",
        "            action_to_take = None\n",
        "\n",
        "            if random.uniform(0,1) < prob_of_best_action: # Take the best action\n",
        "                action_to_take = best_action\n",
        "            else: # Take the other action\n",
        "                action_to_take = 1 if best_action == 0 else 0\n",
        "            \n",
        "            # The agent does the action, and we get the next state, the rewards,\n",
        "            # and whether the game is now done.\n",
        "            next_state, reward, env.done, info = env.step(action_to_take)\n",
        "            \n",
        "            # We now have a state->action->reward sequence we can log\n",
        "            # in `episode`\n",
        "            episode.append((state, action_to_take, reward))\n",
        "            \n",
        "            # update the state for the next decision made by the agent.\n",
        "            state = next_state\n",
        "        \n",
        "    return episode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dIovLcNPX5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_Q(env, episode, Q, alpha, gamma):\n",
        "    '''\n",
        "    THIS IS WHERE THE ALGORITHM HINGES ON BEING FIRST VISIT OR EVERY VISIT.\n",
        "    I AM GOING TO USE FIRST-VISIT, AND HERE'S WHY.\n",
        "    \n",
        "    If you want first-visit, you need to use the cumulative reward of the entire\n",
        "    episode when updating a Q-value for ALL of the state/action pairs in the\n",
        "    episode, even the first state/action pair. In this algorithm, an episode\n",
        "    is a round of Blackjack. Although the bulk of the reward may come from the\n",
        "    2nd or 3rd decision, deciding to hit on the 1st decision is what enabled\n",
        "    the future situations to even occur, so it is important to include the\n",
        "    entire cumulative reward. We can reduce the impact of the rewards of the\n",
        "    future decisions by lowering gamma, which will lower the G value for our\n",
        "    early state/action pair in which we hit and did not get any immediate rewards.\n",
        "    This will make our agent consider future rewards, and not just look at \n",
        "    each state in isolation despite having hit previously.\n",
        "     \n",
        "    If you want Every-Visit MC, do not use the cumulative rewards when updating Q-values,\n",
        "    and just use the immediate reward in this episode for each state/action pair.\n",
        "    '''\n",
        "    step = 0\n",
        "    for state, action, reward in episode:\n",
        "        # calculate the cumulative reward of taking this action in this state.\n",
        "        # Start from the immediate rewards, and use all the rewards from the\n",
        "        # subsequent states. Do not use rewards from previous states.\n",
        "        total_reward = 0\n",
        "        gamma_exp = 0\n",
        "        for curr_step in range(step, len(episode)):\n",
        "            curr_reward = episode[curr_step][2]\n",
        "            total_reward += (gamma ** gamma_exp) * curr_reward\n",
        "            gamma_exp += 1\n",
        "        \n",
        "        # Update the Q-value\n",
        "        Q_state_index = get_Q_state_index(state)\n",
        "        curr_Q_value = Q[Q_state_index][action]\n",
        "        Q[Q_state_index][action] = curr_Q_value + alpha * (total_reward - curr_Q_value)\n",
        "        \n",
        "        # update step to start further down the episode next time.\n",
        "        step += 1\n",
        "        \n",
        "        \n",
        "    return Q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnwidwWg1btk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_Q(env, episode, Q, alpha, gamma):\n",
        "    step = 0\n",
        "    for state, action, reward in episode:\n",
        "        # calculate the cumulative reward of taking this action in this state.\n",
        "        # Start from the immediate rewards, and use all the rewards from the\n",
        "        # subsequent states. Do not use rewards from previous states.\n",
        "        total_reward = 0\n",
        "        gamma_exp = 0\n",
        "        for curr_step in range(step, len(episode)):\n",
        "            curr_reward = episode[curr_step][2]\n",
        "            total_reward += (gamma ** gamma_exp) * curr_reward\n",
        "            gamma_exp += 1\n",
        "        \n",
        "        # Update the Q-value\n",
        "        Q_state_index = get_Q_state_index(state)\n",
        "        curr_Q_value = Q[Q_state_index][action]\n",
        "        Q[Q_state_index][action] = curr_Q_value + alpha * (total_reward - curr_Q_value)\n",
        "        \n",
        "        # update step to start further down the episode next time.\n",
        "        step += 1\n",
        "        \n",
        "        \n",
        "    return Q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhE-gkukPc4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_prob(env, episode, Q, prob, epsilon):\n",
        "    for state, action, reward in episode:\n",
        "        # Update the probabilities of the actions that can be taken given the current\n",
        "        # state. The goal is that the new update in Q has changed what the best action\n",
        "        # is, and epsilon will be used to create a small increase in the probability\n",
        "        # that the new, better action is chosen.\n",
        "        prob = update_prob_of_best_action(env, state, Q, prob, epsilon)\n",
        "        \n",
        "    return prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1NMR26ZPfuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Given a state, derive the corresponding index in the Q-array.\n",
        "# The state is a player hand value + dealer upcard pair,\n",
        "# so a \"hashing\" formula must be used to allocate the\n",
        "# indices of the Q-array properly.\n",
        "def get_Q_state_index(state):\n",
        "    # the player value is already subtracted by 1 in the env when it returns the state.\n",
        "    # subtract by 1 again to fit with the array indexing that starts at 0\n",
        "    initial_player_value = state[0] - 1\n",
        "    # the upcard value is already subtracted by 1 in the env when it returns the state.\n",
        "    # dealer_upcard will be subtracted by 1 to fit with the array indexing that starts at 0\n",
        "    dealer_upcard = state[1] - 1\n",
        "\n",
        "    return (env.observation_space[1].n * (initial_player_value)) + (dealer_upcard)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AGBN7t3Piqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_prob_of_best_action(env, state, Q, prob):\n",
        "    # Use the mapping function to figure out which index of Q corresponds to \n",
        "    # the player hand value + dealer upcard value that defines each state.\n",
        "    Q_state_index = get_Q_state_index(state)\n",
        "    \n",
        "    # Use this index in the Q 2-D array to get a 2-element array that yield\n",
        "    # the current Q-values for hitting (index 0) and standing (index 1) in this state.\n",
        "    # Use the np.argmax() function to find the index of the action that yields the\n",
        "    # rewards i.e. the best action we are looking for.\n",
        "    best_action = np.argmax(Q[Q_state_index])\n",
        "    \n",
        "    # Retrieve the probability of the best action using the \n",
        "    # state/action pair as indices for the `prob` array,\n",
        "    # which stores the probability of taking an action (hit or stand)\n",
        "    # for a given state/action pair.\n",
        "    return prob[Q_state_index][best_action]\n",
        "    \n",
        "def update_prob_of_best_action(env, state, Q, prob, epsilon):\n",
        "\n",
        "    Q_state_index = get_Q_state_index(state)\n",
        "    \n",
        "    best_action = np.argmax(Q[Q_state_index])\n",
        "    \n",
        "    # Slightly alter the probability of this best action being taken by using epsilon\n",
        "    # Epsilon starts at 1.0, and slowly decays over time.\n",
        "    # Therefore, as per the equation below, the AI agent will use the probability listed \n",
        "    # for the best action in the `prob` array during the beginning of the algorithm.\n",
        "    # As time goes on, the likelihood that the best action is taken is increased from\n",
        "    # what is listed in the `prob` array.\n",
        "    # This allows for exploration of other moves in the beginning of the algorithm,\n",
        "    # but exploitation later for a greater reward.\n",
        "    #prob[Q_state_index][best_action] = prob[Q_state_index][best_action] + ((1 - epsilon) * (1 - prob[Q_state_index][best_action]))\n",
        "    prob[Q_state_index][best_action] = min(1, prob[Q_state_index][best_action] + 1 - epsilon)\n",
        "    \n",
        "    other_action = 1 if best_action == 0 else 0\n",
        "    prob[Q_state_index][other_action] = 1 - prob[Q_state_index][best_action]\n",
        "    \n",
        "    return prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH7yB6tLPpNv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = BlackjackEnv()\n",
        "new_Q, new_prob = run_mc(env, 1000000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yozGnhP6PtAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def best_policy(Q):\n",
        "    best_policy_binary = []\n",
        "    best_policy_string = []\n",
        "    best_policy_colors = []\n",
        "    for i in range(len(Q)):\n",
        "        best_policy_binary.append(np.argmax(Q[i]))\n",
        "        best_policy_string.append(\"H\" if np.argmax(Q[i]) == 0 else \"S\")\n",
        "        best_policy_colors.append(\"g\" if np.argmax(Q[i]) == 0 else \"r\")\n",
        "        \n",
        "    return best_policy_binary, best_policy_string, best_policy_colors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YIt4rm1PxTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "new_Q_binary, new_Q_string, new_Q_colors = best_policy(new_Q)\n",
        "\n",
        "df = pd.DataFrame(columns = range(2, 12))\n",
        "\n",
        "color_df = pd.DataFrame(columns = range(2, 12))\n",
        "\n",
        "for s in range(3, 21): # possible player values in the range 3 to 20\n",
        "    start = env.observation_space[1].n * (s-3)\n",
        "    end = start + 10\n",
        "    df.loc[s]=(new_Q_string[start:end])\n",
        "    color_df.loc[s]=(new_Q_colors[start:end])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wta-lkWqPzuP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "0423103f-41e7-41c0-9b0c-ea0eb01975ed"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# hide axes\n",
        "fig.patch.set_visible(False)\n",
        "ax.set_axis_off()\n",
        "ax.axis('tight')\n",
        "\n",
        "ax.table(cellText=df.values, cellColours=color_df.values, cellLoc=\"center\", rowLabels=df.index, colLabels=df.columns, loc='center')\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW6mnnvbP2mf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ab6c19a5-5456-4945-a2f6-f32e97e08af1"
      },
      "source": [
        "import random\n",
        "env = BlackjackEnv()\n",
        "\n",
        "total_rewards = 0\n",
        "NUM_EPISODES = 100000\n",
        "\n",
        "for _ in range(NUM_EPISODES):\n",
        "    state = env.reset()\n",
        "\n",
        "    while env.done == False:\n",
        "        if state[0] == 19: # Player was dealt Blackjack\n",
        "            next_state, reward, env.done, info = env.step(1) # doesn't matter what action is taken.\n",
        "            # don't do any episode analysis for this episode. This is a useless episode.\n",
        "            total_rewards += reward\n",
        "        else:\n",
        "            Q_index = get_Q_state_index(state)\n",
        "            action = new_Q_binary[Q_index]\n",
        "\n",
        "            new_state, reward, done, desc = env.step(action)\n",
        "            state = new_state\n",
        "            total_rewards += reward\n",
        "        \n",
        "avg_reward = total_rewards / NUM_EPISODES\n",
        "print(avg_reward)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-5.358\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}